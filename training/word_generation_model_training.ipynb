{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6hzbl8p91Yl8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#tokeniser - converts text to integers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "#pad_sequences - pad sequences of different lengths with zeros or truncate them to a specified length\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dkfYjt5Y1nRF"
      },
      "outputs": [],
      "source": [
        "# Reading corpus from the text file\n",
        "with open(\"IndiaUS.txt\", 'r', encoding='utf-8') as myfile:\n",
        "    mytext = myfile.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZzxSWYj1tN5"
      },
      "outputs": [],
      "source": [
        "print(mytext)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0WncB1I12GRI"
      },
      "outputs": [],
      "source": [
        "mytokenizer = Tokenizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2-X57qld1_1I"
      },
      "outputs": [],
      "source": [
        "mytokenizer.fit_on_texts([mytext])\n",
        "total_words = len(mytokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UivYRzFE2Hy4",
        "outputId": "f560d505-de86-41d7-82b5-cf5e3797ffeb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'to': 2,\n",
              " 'in': 3,\n",
              " 'a': 4,\n",
              " 'and': 5,\n",
              " 'india': 6,\n",
              " 'of': 7,\n",
              " 'mr': 8,\n",
              " 'us': 9,\n",
              " 'is': 10,\n",
              " 'modi': 11,\n",
              " 'has': 12,\n",
              " 'that': 13,\n",
              " 'biden': 14,\n",
              " 'as': 15,\n",
              " 'with': 16,\n",
              " 'not': 17,\n",
              " \"india's\": 18,\n",
              " 'for': 19,\n",
              " 'but': 20,\n",
              " 'on': 21,\n",
              " 'washington': 22,\n",
              " 'an': 23,\n",
              " 'it': 24,\n",
              " 'says': 25,\n",
              " 'will': 26,\n",
              " 'are': 27,\n",
              " 'indian': 28,\n",
              " 'prime': 29,\n",
              " 'minister': 30,\n",
              " 'this': 31,\n",
              " 'have': 32,\n",
              " 'trade': 33,\n",
              " 'visit': 34,\n",
              " 'by': 35,\n",
              " 'president': 36,\n",
              " 'relationship': 37,\n",
              " 'been': 38,\n",
              " 'about': 39,\n",
              " 'strategic': 40,\n",
              " 'more': 41,\n",
              " 'up': 42,\n",
              " 'during': 43,\n",
              " 'also': 44,\n",
              " 'make': 45,\n",
              " 'sirohi': 46,\n",
              " 'china': 47,\n",
              " 'air': 48,\n",
              " 'force': 49,\n",
              " 'state': 50,\n",
              " 'his': 51,\n",
              " 'ties': 52,\n",
              " 'two': 53,\n",
              " 'at': 54,\n",
              " 'white': 55,\n",
              " 'house': 56,\n",
              " 'be': 57,\n",
              " 'lot': 58,\n",
              " 'ms': 59,\n",
              " 'technology': 60,\n",
              " 'jet': 61,\n",
              " 'semiconductor': 62,\n",
              " 'now': 63,\n",
              " 'first': 64,\n",
              " 'was': 65,\n",
              " 'global': 66,\n",
              " 'one': 67,\n",
              " 'he': 68,\n",
              " 'without': 69,\n",
              " 'they': 70,\n",
              " 'narendra': 71,\n",
              " 'joe': 72,\n",
              " 'world': 73,\n",
              " 'potential': 74,\n",
              " 'how': 75,\n",
              " 'become': 76,\n",
              " 'kugelman': 77,\n",
              " 'years': 78,\n",
              " 'there': 79,\n",
              " \"modi's\": 80,\n",
              " 'fighter': 81,\n",
              " 'from': 82,\n",
              " 'which': 83,\n",
              " 'russia': 84,\n",
              " 'sides': 85,\n",
              " '2023': 86,\n",
              " 'seen': 87,\n",
              " 'under': 88,\n",
              " 'administration': 89,\n",
              " 'or': 90,\n",
              " 'way': 91,\n",
              " 'policy': 92,\n",
              " 'partner': 93,\n",
              " 'autonomy': 94,\n",
              " \"didn't\": 95,\n",
              " 'could': 96,\n",
              " 'joint': 97,\n",
              " 'major': 98,\n",
              " 'following': 99,\n",
              " \"country's\": 100,\n",
              " 'partnership': 101,\n",
              " 'most': 102,\n",
              " 'strengthening': 103,\n",
              " 'between': 104,\n",
              " 'nations': 105,\n",
              " 'closer': 106,\n",
              " 'than': 107,\n",
              " 'time': 108,\n",
              " 'said': 109,\n",
              " 'may': 110,\n",
              " 'broad': 111,\n",
              " 'reason': 112,\n",
              " 'can': 113,\n",
              " \"china's\": 114,\n",
              " 'indo': 115,\n",
              " 'pacific': 116,\n",
              " 'had': 117,\n",
              " 'because': 118,\n",
              " 'later': 119,\n",
              " 'purchase': 120,\n",
              " 'former': 121,\n",
              " 'general': 122,\n",
              " 'owned': 123,\n",
              " 'only': 124,\n",
              " 'wants': 125,\n",
              " 'arms': 126,\n",
              " 'sharing': 127,\n",
              " 'military': 128,\n",
              " 'drones': 129,\n",
              " 'facility': 130,\n",
              " 'into': 131,\n",
              " 'biggest': 132,\n",
              " \"washington's\": 133,\n",
              " 'invest': 134,\n",
              " 'test': 135,\n",
              " 'maker': 136,\n",
              " 'research': 137,\n",
              " 'all': 138,\n",
              " 'future': 139,\n",
              " 'u': 140,\n",
              " 's': 141,\n",
              " 'welcome': 142,\n",
              " 'june': 143,\n",
              " 'dc': 144,\n",
              " 'many': 145,\n",
              " 'never': 146,\n",
              " 'place': 147,\n",
              " 'foreign': 148,\n",
              " 'wanted': 149,\n",
              " 'what': 150,\n",
              " 'some': 151,\n",
              " 'relations': 152,\n",
              " 'close': 153,\n",
              " 'obama': 154,\n",
              " 'trump': 155,\n",
              " 'further': 156,\n",
              " 'did': 157,\n",
              " 'importance': 158,\n",
              " 'free': 159,\n",
              " 'united': 160,\n",
              " 'states': 161,\n",
              " 'usaf': 162,\n",
              " 'personnel': 163,\n",
              " 'front': 164,\n",
              " 'april': 165,\n",
              " 'full': 166,\n",
              " 'over': 167,\n",
              " 'tariffs': 168,\n",
              " 'were': 169,\n",
              " 'discussions': 170,\n",
              " 'disputes': 171,\n",
              " \"it's\": 172,\n",
              " 'democratic': 173,\n",
              " 'hindu': 174,\n",
              " 'party': 175,\n",
              " 'mutual': 176,\n",
              " 'lavish': 177,\n",
              " 'called': 178,\n",
              " 'among': 179,\n",
              " 'consequential': 180,\n",
              " \"bbc's\": 181,\n",
              " 'vikas': 182,\n",
              " 'pandey': 183,\n",
              " 'soutik': 184,\n",
              " 'biswas': 185,\n",
              " 'explore': 186,\n",
              " 'factors': 187,\n",
              " 'contribute': 188,\n",
              " \"visit's\": 189,\n",
              " \"us's\": 190,\n",
              " \"world's\": 191,\n",
              " 'populous': 192,\n",
              " 'country': 193,\n",
              " 'stronger': 194,\n",
              " 'dynamic': 195,\n",
              " 'any': 196,\n",
              " 'history': 197,\n",
              " 'completion': 198,\n",
              " 'pomp': 199,\n",
              " 'filled': 200,\n",
              " 'remark': 201,\n",
              " 'exaggeration': 202,\n",
              " 'summit': 203,\n",
              " 'suggests': 204,\n",
              " 'transformed': 205,\n",
              " 'underscores': 206,\n",
              " 'just': 207,\n",
              " 'deep': 208,\n",
              " 'relatively': 209,\n",
              " 'short': 210,\n",
              " 'michael': 211,\n",
              " 'wilson': 212,\n",
              " 'center': 213,\n",
              " 'american': 214,\n",
              " 'think': 215,\n",
              " 'tank': 216,\n",
              " 'key': 217,\n",
              " 'keen': 218,\n",
              " 'draw': 219,\n",
              " 'so': 220,\n",
              " 'act': 221,\n",
              " 'counterbalance': 222,\n",
              " 'growing': 223,\n",
              " 'influence': 224,\n",
              " 'lived': 225,\n",
              " 'their': 226,\n",
              " 'promise': 227,\n",
              " 'landmark': 228,\n",
              " 'civilian': 229,\n",
              " 'nuclear': 230,\n",
              " 'deal': 231,\n",
              " '2005': 232,\n",
              " 'liability': 233,\n",
              " 'law': 234,\n",
              " 'passed': 235,\n",
              " 'three': 236,\n",
              " 'hobbled': 237,\n",
              " 'reactors': 238,\n",
              " 'followed': 239,\n",
              " 'fading': 240,\n",
              " 'commitment': 241,\n",
              " 'manmohan': 242,\n",
              " \"singh's\": 243,\n",
              " 'second': 244,\n",
              " 'term': 245,\n",
              " 'leader': 246,\n",
              " 'coalition': 247,\n",
              " 'government': 248,\n",
              " 'enthusiasm': 249,\n",
              " 'embracing': 250,\n",
              " 'given': 251,\n",
              " 'overall': 252,\n",
              " 'directive': 253,\n",
              " 'work': 254,\n",
              " 'seema': 255,\n",
              " 'author': 256,\n",
              " 'friends': 257,\n",
              " 'benefits': 258,\n",
              " 'story': 259,\n",
              " 'put': 260,\n",
              " 'effort': 261,\n",
              " 'substantive': 262,\n",
              " 'deliverables': 263,\n",
              " 'defence': 264,\n",
              " 'industrial': 265,\n",
              " 'cooperation': 266,\n",
              " 'topped': 267,\n",
              " 'list': 268,\n",
              " 'consider': 269,\n",
              " 'electric': 270,\n",
              " 'hindustan': 271,\n",
              " 'aeronautics': 272,\n",
              " 'limited': 273,\n",
              " 'advanced': 274,\n",
              " 'engines': 275,\n",
              " 'indigenous': 276,\n",
              " 'light': 277,\n",
              " 'combat': 278,\n",
              " 'aircraft': 279,\n",
              " 'means': 280,\n",
              " 'greater': 281,\n",
              " 'transfer': 282,\n",
              " 'engine': 283,\n",
              " 'ever': 284,\n",
              " 'before': 285,\n",
              " 'clear': 286,\n",
              " 'sign': 287,\n",
              " 'sell': 288,\n",
              " 'comfortable': 289,\n",
              " 'proceed': 290,\n",
              " '3bn': 291,\n",
              " 'battle': 292,\n",
              " 'tested': 293,\n",
              " 'mq': 294,\n",
              " '9b': 295,\n",
              " 'predator': 296,\n",
              " 'atomics': 297,\n",
              " 'set': 298,\n",
              " 'assembled': 299,\n",
              " 'fits': 300,\n",
              " \"'make\": 301,\n",
              " \"india'\": 302,\n",
              " 'campaign': 303,\n",
              " 'supplies': 304,\n",
              " '11': 305,\n",
              " '45': 306,\n",
              " 'supplier': 307,\n",
              " 'hopes': 308,\n",
              " 'primary': 309,\n",
              " 'provider': 310,\n",
              " 'coming': 311,\n",
              " 'immediate': 312,\n",
              " 'goal': 313,\n",
              " 'strengthen': 314,\n",
              " 'capacity': 315,\n",
              " 'counter': 316,\n",
              " 'base': 317,\n",
              " 'memory': 318,\n",
              " 'chip': 319,\n",
              " 'giant': 320,\n",
              " 'micron': 321,\n",
              " '825m': 322,\n",
              " 'build': 323,\n",
              " 'assembly': 324,\n",
              " 'creating': 325,\n",
              " 'thousands': 326,\n",
              " 'jobs': 327,\n",
              " 'equipment': 328,\n",
              " 'lam': 329,\n",
              " 'train': 330,\n",
              " '60': 331,\n",
              " '000': 332,\n",
              " 'engineers': 333,\n",
              " 'through': 334,\n",
              " 'network': 335,\n",
              " 'interconnected': 336,\n",
              " 'labs': 337,\n",
              " 'centres': 338,\n",
              " 'speed': 339,\n",
              " 'education': 340,\n",
              " 'workforce': 341,\n",
              " 'development': 342,\n",
              " 'applied': 343,\n",
              " 'materials': 344,\n",
              " 'machines': 345,\n",
              " 'producing': 346,\n",
              " 'semiconductors': 347,\n",
              " '400m': 348,\n",
              " 'establish': 349,\n",
              " 'engineering': 350,\n",
              " 'centre': 351,\n",
              " 'both': 352,\n",
              " 'talking': 353,\n",
              " 'cutting': 354,\n",
              " 'edge': 355,\n",
              " 'technologies': 356,\n",
              " 'seed': 357,\n",
              " 'shape': 358,\n",
              " 'lady': 359,\n",
              " 'jill': 360,\n",
              " '21': 361,\n",
              " 'mrs': 362,\n",
              " 'ups': 363,\n",
              " 'downs': 364,\n",
              " 'since': 365,\n",
              " 'seriously': 366,\n",
              " 'began': 367,\n",
              " 'courting': 368,\n",
              " 'bill': 369,\n",
              " 'clinton': 370,\n",
              " 'then': 371,\n",
              " 'george': 372,\n",
              " 'bush': 373,\n",
              " 'response': 374,\n",
              " 'measured': 375,\n",
              " 'overeager': 376,\n",
              " 'too': 377,\n",
              " 'forthcoming': 378,\n",
              " 'saw': 379,\n",
              " 'geopolitics': 380,\n",
              " 'its': 381,\n",
              " 'own': 382,\n",
              " 'order': 383,\n",
              " 'strategy': 384,\n",
              " 'nonalignment': 385,\n",
              " 'started': 386,\n",
              " 'jawaharlal': 387,\n",
              " 'nehru': 388,\n",
              " 'always': 389,\n",
              " 'deeply': 390,\n",
              " 'rooted': 391,\n",
              " 'camp': 392,\n",
              " 'other': 393,\n",
              " 'junior': 394,\n",
              " 'superpower': 395,\n",
              " 'left': 396,\n",
              " 'ideals': 397,\n",
              " 'describe': 398,\n",
              " 'altruism': 399,\n",
              " 'leading': 400,\n",
              " 'different': 401,\n",
              " 'kind': 402,\n",
              " 'considerably': 403,\n",
              " 'economic': 404,\n",
              " 'geopolitical': 405,\n",
              " 'heft': 406,\n",
              " 'formed': 407,\n",
              " 'bonds': 408,\n",
              " 'presidents': 409,\n",
              " 'barack': 410,\n",
              " 'donald': 411,\n",
              " 'sacrificed': 412,\n",
              " 'would': 413,\n",
              " 'go': 414,\n",
              " 'step': 415,\n",
              " 'probably': 416,\n",
              " 'take': 417,\n",
              " 'harder': 418,\n",
              " 'public': 419,\n",
              " 'stand': 420,\n",
              " 'seem': 421,\n",
              " 'disappointed': 422,\n",
              " 'repeated': 423,\n",
              " 'line': 424,\n",
              " 'era': 425,\n",
              " 'war': 426,\n",
              " 'mentioning': 427,\n",
              " 'speak': 428,\n",
              " 'beefing': 429,\n",
              " 'humanitarian': 430,\n",
              " 'assistance': 431,\n",
              " 'ukraine': 432,\n",
              " 'mention': 433,\n",
              " 'name': 434,\n",
              " 'either': 435,\n",
              " 'talk': 436,\n",
              " 'prosperous': 437,\n",
              " 'far': 438,\n",
              " 'pushed': 439,\n",
              " \"administration's\": 440,\n",
              " 'compromising': 441,\n",
              " 'ideal': 442,\n",
              " 'come': 443,\n",
              " 'making': 444,\n",
              " 'success': 445,\n",
              " 'iaf': 446,\n",
              " 'posing': 447,\n",
              " 'f': 448,\n",
              " '15': 449,\n",
              " 'eagle': 450,\n",
              " \"'exercise\": 451,\n",
              " 'cope': 452,\n",
              " \"2023'\": 453,\n",
              " 'station': 454,\n",
              " 'kalaikunda': 455,\n",
              " 'around': 456,\n",
              " '170': 457,\n",
              " 'km': 458,\n",
              " 'west': 459,\n",
              " 'kolkata': 460,\n",
              " '24th': 461,\n",
              " 'pose': 462,\n",
              " 'exercise': 463,\n",
              " 'militaries': 464,\n",
              " 'working': 465,\n",
              " 'closely': 466,\n",
              " 'together': 467,\n",
              " 'arrangements': 468,\n",
              " 'where': 469,\n",
              " 'use': 470,\n",
              " 'each': 471,\n",
              " \"other's\": 472,\n",
              " 'facilities': 473,\n",
              " 'refuelling': 474,\n",
              " 'maintenance': 475,\n",
              " 'purposes': 476,\n",
              " 'holding': 477,\n",
              " 'exercises': 478,\n",
              " \"they're\": 479,\n",
              " 'intelligence': 480,\n",
              " 'credit': 481,\n",
              " 'managing': 482,\n",
              " 'really': 483,\n",
              " 'limits': 484,\n",
              " 'sense': 485,\n",
              " 'getting': 486,\n",
              " 'you': 487,\n",
              " 'power': 488,\n",
              " 'signing': 489,\n",
              " 'fledged': 490,\n",
              " 'alliance': 491,\n",
              " 'differences': 492,\n",
              " 'recent': 493,\n",
              " 'particularly': 494,\n",
              " 'suffered': 495,\n",
              " 'expected': 496,\n",
              " 'announce': 497,\n",
              " 'anything': 498,\n",
              " 'understood': 499,\n",
              " 'continue': 500,\n",
              " 'overshadowing': 501,\n",
              " 'surprisingly': 502,\n",
              " 'announced': 503,\n",
              " 'six': 504,\n",
              " 'separate': 505,\n",
              " 'organization': 506,\n",
              " 'resolved': 507,\n",
              " 'including': 508,\n",
              " 'involved': 509,\n",
              " 'top': 510,\n",
              " 'trading': 511,\n",
              " '130bn': 512,\n",
              " 'goods': 513,\n",
              " 'delhi': 514,\n",
              " 'eighth': 515,\n",
              " 'largest': 516,\n",
              " 'while': 517,\n",
              " 'these': 518,\n",
              " 'numbers': 519,\n",
              " 'impressive': 520,\n",
              " 'analysts': 521,\n",
              " 'policymakers': 522,\n",
              " 'feel': 523,\n",
              " 'huge': 524,\n",
              " 'untapped': 525,\n",
              " 'burgeoning': 526,\n",
              " 'market': 527,\n",
              " 'expanding': 528,\n",
              " 'middle': 529,\n",
              " 'class': 530,\n",
              " 'positioning': 531,\n",
              " 'itself': 532,\n",
              " 'alternative': 533,\n",
              " 'manufacturing': 534,\n",
              " 'hub': 535,\n",
              " 'participate': 536,\n",
              " 'arrival': 537,\n",
              " 'ceremony': 538,\n",
              " 'south': 539,\n",
              " 'lawn': 540,\n",
              " 'thursday': 541,\n",
              " '22': 542,\n",
              " 'invite': 543,\n",
              " 'official': 544,\n",
              " '2': 545,\n",
              " '7': 546,\n",
              " 'million': 547,\n",
              " 'indians': 548,\n",
              " 'live': 549,\n",
              " 'firms': 550,\n",
              " 'interested': 551,\n",
              " 'proposal': 552,\n",
              " 'look': 553,\n",
              " 'supply': 554,\n",
              " 'chain': 555,\n",
              " 'dominance': 556,\n",
              " 'context': 557,\n",
              " 'resolution': 558,\n",
              " 'give': 559,\n",
              " 'impetus': 560,\n",
              " 'unlocking': 561,\n",
              " 'even': 562,\n",
              " 'sky': 563,\n",
              " 'limit': 564,\n",
              " 'critics': 565,\n",
              " 'questioned': 566,\n",
              " 'backsliding': 567,\n",
              " 'nationalist': 568,\n",
              " 'bharatiya': 569,\n",
              " 'janata': 570,\n",
              " 'bjp': 571,\n",
              " 'television': 572,\n",
              " 'interview': 573,\n",
              " 'week': 574,\n",
              " 'emphasised': 575,\n",
              " 'significance': 576,\n",
              " 'addressing': 577,\n",
              " 'protection': 578,\n",
              " 'muslim': 579,\n",
              " 'minority': 580,\n",
              " 'predominantly': 581,\n",
              " 'progressives': 582,\n",
              " 'disturbed': 583,\n",
              " 'happening': 584,\n",
              " 'realists': 585,\n",
              " 'centrists': 586,\n",
              " 'factor': 587,\n",
              " 'whole': 588,\n",
              " 'bipartisan': 589,\n",
              " 'agreement': 590,\n",
              " 'deeper': 591,\n",
              " 'broader': 592,\n",
              " 'certainly': 593,\n",
              " 'moved': 594,\n",
              " 'next': 595,\n",
              " 'level': 596,\n",
              " 'need': 597,\n",
              " 'benefit': 598}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mytokenizer.word_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ypH9ex9F2Ktg"
      },
      "outputs": [],
      "source": [
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(mytokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "n-gram sequence:\n",
        " contiguous sequence of n items from a given sample of text \n",
        "\n",
        "\"I love machine learning\"\n",
        "\n",
        "list = [1, 2, 3, 4]\n",
        "bigram  : \"i love\",\"love machine\",\"machine learning\"\n",
        "[1,2]\n",
        "[1,2,3]\n",
        "[1,2,3,4]\n",
        "\n",
        "trigram : \"i love machine\",love machine learning\"\n",
        "[1,2,3]\n",
        "[1,2,3,4]\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XOqiXEIs2RhA"
      },
      "outputs": [],
      "source": [
        "my_input_sequences = []\n",
        "for line in mytext.split('\\n'):\n",
        "    #tokenizing the text using the texts_to_sequences,\n",
        "    # converts the text line into a list of integer tokens\n",
        "    token_list = mytokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        #creating n-gram :\n",
        "        #i+1 means 2-grams(bigrams)\n",
        "        #i+2 means 3-grams(trigrams)\n",
        "        my_n_gram_sequence = token_list[:i+1]\n",
        "        my_input_sequences.append(my_n_gram_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oTxGBnkK2UGw"
      },
      "outputs": [],
      "source": [
        "#store the maximum sequence length among all the sequences\n",
        "max_sequence_len = max([len(seq) for seq in my_input_sequences])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "''' \n",
        "Padding\n",
        "--------------------\n",
        "\n",
        "-> max_sequence_len value can be useful when padding sequences to ensure they are all of the same length.\n",
        "-> Padding involves adding dummy elements (usually zeros) to sequences that are shorter than a predefined length.\n",
        "   This ensures that all sequences have the same length, which is often required when training neural networks or\n",
        "   performing batch operations\n",
        "-> To train a batch of sequences in (Neural networks), they need to have the same dimensions\n",
        "-> LSTM or RNN, require input sequences of the same length.\n",
        "   This is because the network's architecture assumes a fixed input size.\n",
        "\n",
        "   my_input_sequences = [[1, 2, 3],\n",
        "                        [4, 5],\n",
        "                        [6, 7, 8, 9]]\n",
        "\n",
        "   after padding with max_sequence_len = 4 (last list length)\n",
        "\n",
        "   input_sequences    = [[0, 1, 2, 3],\n",
        "                        [0, 0, 4, 5],\n",
        "                        [6, 7, 8, 9]]\n",
        "now its all are same length\n",
        "This padded array is now suitable for feeding into a neural network model for training.\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "l1OtyGsK2ioo"
      },
      "outputs": [],
      "source": [
        "\n",
        "#padding='pre' = padding is added to the beginning (prefix) of the sequences\n",
        "# padding='post' padding is added to the end of the sequences\n",
        "input_sequences = np.array(pad_sequences(my_input_sequences, maxlen=max_sequence_len, padding='pre'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp0iMEuh2kJw",
        "outputId": "a1011688-9ece-42c1-83b1-145e6bc35149"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,  99,   4, 177], dtype=int32)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sequences[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2-UvkLy12qwf"
      },
      "outputs": [],
      "source": [
        "# includes all rows (:) and all columns except the last one (:-1).\n",
        "# This means that X will consist of the entire sequence except for the last element in each sequence(which need to be predicted)\n",
        "X = input_sequences[:, :-1]\n",
        "\n",
        "#  y is being assigned the last column (-1) of input_sequences last element in each sequence (target to predict)\n",
        "y = input_sequences[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "The data is now structured in a way that you can use it for supervised training.\n",
        "X will be your input data, and y will be your corresponding target labels\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rneRsVT62tbP",
        "outputId": "a30a5421-8c0e-42ff-a908-f6e3ba6e6469"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 99,  4],\n",
              "      dtype=int32)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "651i5zlw2ynX",
        "outputId": "8c2135b3-dac4-466b-9dff-c065d12b1a8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  4, 177,  50, ...,  25,  59,  46], dtype=int32)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TdgRjm9I2zav"
      },
      "outputs": [],
      "source": [
        "#  converting the target values y into one-hot encoded vectors using the to_categorical\n",
        "# Each value in y is transformed into a binary vector of length total_words,\n",
        "# where the position corresponding to the target value is set to 1 and the rest are set to 0.\n",
        "'''\n",
        "In one-hot encoding:\n",
        "\n",
        "Each category is represented by a unique binary vector.\n",
        "The length of the vector is equal to the total number of unique categories (classes).\n",
        "Only one element in the vector is set to 1 (hot), and the rest are set to 0 (cold).\n",
        "\n",
        "the target values in y are being treated as categories, and you're converting them\n",
        "to one-hot encoded vectors of length total_words. The position corresponding to the target \n",
        "value is set to 1, and the rest are set to 0.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6QyWhaI21bf"
      },
      "outputs": [],
      "source": [
        "y[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8Xr9iIms22sn"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "# embedding layer to map words to vectors\n",
        "# The Embedding layer is used to create word embeddings that map words from a \n",
        "# high-dimensional space (the vocabulary size) to a lower-dimensional space (embedding dimension)\n",
        "    # total_words: number of unique words in your vocabulary, which defines the size of the input vocabulary.\n",
        "    # 100: The dimensionality of the word embeddings. This is a hyperparameter you can adjust based on your needs.\n",
        "    # input_length=max_sequence_len-1: The length of the input sequences (excluding the target word) after padding.\n",
        "\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YsDsPKiO26dH"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVRNQdip29Fn"
      },
      "outputs": [],
      "source": [
        "model.fit(X, y, epochs=100, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tAx36gI22-7f"
      },
      "outputs": [],
      "source": [
        "model.save('word_generation_model.h5')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y92OW3Jk3WpP",
        "outputId": "f1c36d66-0e71-4fee-b535-b7bfbd13f802"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 384ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "narendra modi with india a semiconductor base us memory chip giant micron technology\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "# Load the tokenizer from training\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    mytokenizer = pickle.load(handle)\n",
        "\n",
        "# Load the trained model\n",
        "model = tf.keras.models.load_model('word_generation_model.h5')\n",
        "\n",
        "input_text = \"narendra modi with\"\n",
        "predict_next_words = 10\n",
        "\n",
        "for _ in range(predict_next_words):\n",
        "    token_list = mytokenizer.texts_to_sequences([input_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=model.input_shape[1], padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "    output_word = mytokenizer.index_word[predicted[0]]  # Convert index to word using tokenizer\n",
        "    input_text += \" \" + output_word\n",
        "\n",
        "print(input_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9-EMoXB3Zsu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
